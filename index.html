<!DOCTYPE html>
<html>
<head>
    <title>Color Analysis</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>
        /* Modern, clean design */
        body { 
            margin: 0; 
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Arial;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        }
        #video-container {
            position: relative;
            max-width: 640px;
            margin: 20px auto;
        }
        video, canvas {
            width: 100%;
            border-radius: 20px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
        }
        /* Face oval overlay */
        .face-guide {
            position: absolute;
            top: 20%;
            left: 50%;
            transform: translateX(-50%);
            width: 60%;
            height: 50%;
            border: 3px solid #00ff88;
            border-radius: 50%;
            pointer-events: none;
        }
    </style>
</head>
<body>
    <div id="app">
        <div id="video-container">
            <video id="video" autoplay playsinline></video>
            <canvas id="canvas"></canvas>
            <div class="face-guide"></div>
        </div>
        
        <div id="instructions">
            Hold white post-it in view and tap it
        </div>
        
        <div id="countdown"></div>
        
        <div id="results" style="display:none;">
            <img id="corrected-image" />
            <button id="download">ğŸ“¥ Download Corrected Image</button>
            <div id="colors"></div>
        </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/body-pix"></script>
    
    <script>
        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// STEP 1: Camera Setup
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

const video = document.getElementById('video');
const canvas = document.getElementById('canvas');
const ctx = canvas.getContext('2d');

let referenceLocation = null;
let referenceColor = null;

async function startCamera() {
    const stream = await navigator.mediaDevices.getUserMedia({
        video: {
            facingMode: 'user',
            width: { ideal: 1280 },
            height: { ideal: 720 }
        }
    });
    
    video.srcObject = stream;
    video.play();
    
    // Set canvas size to match video
    video.onloadedmetadata = () => {
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
    };
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// STEP 2: User Taps White Reference
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

video.addEventListener('click', (e) => {
    if (referenceLocation) return; // Already locked
    
    const rect = video.getBoundingClientRect();
    const x = (e.clientX - rect.left) * (video.videoWidth / rect.width);
    const y = (e.clientY - rect.top) * (video.videoHeight / rect.height);
    
    // Sample 50x50 pixel area around tap
    ctx.drawImage(video, 0, 0);
    const imageData = ctx.getImageData(x - 25, y - 25, 50, 50);
    
    // Get average color of tapped area
    referenceColor = getAverageColor(imageData);
    
    // Validate it's actually white-ish
    if (isValidWhite(referenceColor)) {
        referenceLocation = { x, y };
        
        // Try to lock camera settings (might not work in browser)
        tryLockCameraSettings(referenceColor);
        
        // Show countdown
        startCountdown();
    } else {
        alert('Please tap on a white surface!');
    }
});

function getAverageColor(imageData) {
    const data = imageData.data;
    let r = 0, g = 0, b = 0, count = 0;
    
    for (let i = 0; i < data.length; i += 4) {
        r += data[i];
        g += data[i + 1];
        b += data[i + 2];
        count++;
    }
    
    return {
        r: r / count,
        g: g / count,
        b: b / count
    };
}

function isValidWhite(color) {
    // Check if color is bright and neutral
    const brightness = (color.r + color.g + color.b) / 3;
    const variance = Math.max(
        Math.abs(color.r - brightness),
        Math.abs(color.g - brightness),
        Math.abs(color.b - brightness)
    );
    
    return brightness > 180 && variance < 30;
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// STEP 3: Countdown and Capture
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

function startCountdown() {
    const countdownEl = document.getElementById('countdown');
    let count = 3;
    
    const interval = setInterval(() => {
        countdownEl.textContent = count;
        countdownEl.style.fontSize = '100px';
        
        if (count === 0) {
            clearInterval(interval);
            capturePhoto();
        }
        count--;
    }, 1000);
}

function capturePhoto() {
    // Capture current frame with reference visible
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
    
    // Get image data for processing
    const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
    
    // Process the image
    processImage(imageData);
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// STEP 4: Color Correction Using Reference
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

function processImage(imageData) {
    // Find the white reference in the image
    const referenceInImage = findReferenceInImage(imageData, referenceLocation);
    
    if (!referenceInImage) {
        alert('Reference not found in photo. Please try again.');
        return;
    }
    
    // Calculate color correction
    const correction = calculateCorrection(referenceInImage);
    
    // Apply correction to entire image
    const correctedImageData = applyCorrection(imageData, correction);
    
    // Draw corrected image
    ctx.putImageData(correctedImageData, 0, 0);
    
    // Convert to downloadable image
    const correctedImage = canvas.toDataURL('image/jpeg', 0.95);
    
    // Extract face colors from corrected image
    extractFaceColors(correctedImageData).then(colors => {
        showResults(correctedImage, colors);
    });
}

function findReferenceInImage(imageData, location) {
    // Sample area around where user tapped
    const x = Math.floor(location.x);
    const y = Math.floor(location.y);
    const size = 50;
    
    const data = imageData.data;
    const width = imageData.width;
    
    let r = 0, g = 0, b = 0, count = 0;
    
    for (let dy = -size/2; dy < size/2; dy++) {
        for (let dx = -size/2; dx < size/2; dx++) {
            const px = x + dx;
            const py = y + dy;
            if (px >= 0 && px < width && py >= 0 && py < imageData.height) {
                const i = (py * width + px) * 4;
                r += data[i];
                g += data[i + 1];
                b += data[i + 2];
                count++;
            }
        }
    }
    
    return {
        r: r / count,
        g: g / count,
        b: b / count
    };
}

function calculateCorrection(measuredWhite) {
    // Target: Pure white (255, 255, 255)
    // Measured: What the camera actually captured
    
    // Calculate per-channel gains
    const targetWhite = 255;
    
    return {
        rGain: targetWhite / measuredWhite.r,
        gGain: targetWhite / measuredWhite.g,
        bGain: targetWhite / measuredWhite.b
    };
}

function applyCorrection(imageData, correction) {
    const corrected = ctx.createImageData(imageData.width, imageData.height);
    const src = imageData.data;
    const dst = corrected.data;
    
    for (let i = 0; i < src.length; i += 4) {
        // Apply gains with clamping
        dst[i]     = Math.min(255, Math.max(0, src[i] * correction.rGain));
        dst[i + 1] = Math.min(255, Math.max(0, src[i + 1] * correction.gGain));
        dst[i + 2] = Math.min(255, Math.max(0, src[i + 2] * correction.bGain));
        dst[i + 3] = src[i + 3]; // Alpha unchanged
    }
    
    return corrected;
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// STEP 5: Face Color Extraction
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async function extractFaceColors(imageData) {
    // Load face segmentation model
    const bodyPix = await bodySegmentation.createSegmenter(
        bodySegmentation.SupportedModels.BodyPix
    );
    
    // Create image from imageData
    const tempCanvas = document.createElement('canvas');
    tempCanvas.width = imageData.width;
    tempCanvas.height = imageData.height;
    const tempCtx = tempCanvas.getContext('2d');
    tempCtx.putImageData(imageData, 0, 0);
    
    // Segment the image
    const segmentation = await bodyPix.segmentPerson(tempCanvas);
    
    // Extract colors from face regions
    const colors = {
        skin: extractRegionColor(imageData, segmentation, 'face'),
        hair: extractRegionColor(imageData, segmentation, 'hair'),
        // eyes: more complex, needs eye detection
    };
    
    return colors;
}

function extractRegionColor(imageData, segmentation, region) {
    // Sample multiple pixels from region
    // Remove outliers
    // Return median color
    
    // Simplified version:
    const data = imageData.data;
    const mask = segmentation.data;
    
    let r = 0, g = 0, b = 0, count = 0;
    
    for (let i = 0; i < mask.length; i++) {
        if (mask[i] === 1) { // Person pixel
            const idx = i * 4;
            r += data[idx];
            g += data[idx + 1];
            b += data[idx + 2];
            count++;
        }
    }
    
    return {
        r: Math.round(r / count),
        g: Math.round(g / count),
        b: Math.round(b / count)
    };
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// STEP 6: Show Results
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

function showResults(correctedImage, colors) {
    document.getElementById('video-container').style.display = 'none';
    document.getElementById('results').style.display = 'block';
    
    // Show corrected image
    const img = document.getElementById('corrected-image');
    img.src = correctedImage;
    
    // Show color swatches
    const colorsDiv = document.getElementById('colors');
    colorsDiv.innerHTML = `
        <div class="color-result">
            <div class="swatch" style="background: rgb(${colors.skin.r}, ${colors.skin.g}, ${colors.skin.b})"></div>
            <span>Skin Tone</span>
        </div>
        <div class="color-result">
            <div class="swatch" style="background: rgb(${colors.hair.r}, ${colors.hair.g}, ${colors.hair.b})"></div>
            <span>Hair Color</span>
        </div>
    `;
    
    // Download button
    document.getElementById('download').onclick = () => {
        const link = document.createElement('a');
        link.download = 'color-corrected.jpg';
        link.href = correctedImage;
        link.click();
    };
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// START THE APP
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

startCamera();
    </script>
</body>
</html>
